{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Anomaly Detection: make_blobs data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import OneClassSVM\n\nX, y_true = make_blobs(n_samples=500, \n                       centers=5,\n                       cluster_std=0.80, \n                       random_state=0)\n\nX = StandardScaler().fit_transform(X)\n\nfig = plt.figure(figsize=(6,5))\nplt.scatter(X[:,0], X[:,1], s=10, color='k')\nplt.axis([-2.5, 2.5, -2.5, 2.5])\nplt.title('Original Observations')\nplt.grid()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Perform Elliptic Envelope",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make a meshgrid for plotting surfaces\nXp, Yp = np.meshgrid(np.linspace(-2.5,2.5),np.linspace(-2.5,2.5))\nXY = np.vstack([Xp.ravel(), Yp.ravel()]).T\n\nenvelope = EllipticEnvelope(random_state=0, contamination=0.1).fit(X)\nZp = envelope.score_samples(XY)\nZp = Zp.reshape(Xp.shape)\n\n# Get the anomalous data points\ny_pred = envelope.predict(X)\nnormals = X[y_pred == 1,:]\nanomals = X[y_pred == -1,:]\n\ncntr = plt.contourf(Xp, Yp, Zp, levels=10, cmap='viridis')\nplt.scatter(normals[:,0], normals[:,1], s=5, color='k', label='Normal Observations')\nplt.scatter(anomals[:,0], anomals[:,1], s=5, color='r', label='Anomalies (Contamination = 0.1)')\nplt.title('Anomaly Detection using Elliptic Envelope')\nplt.colorbar(cntr)\nplt.legend()\nplt.grid()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The Elliptic Envelope assumes that the data has a multivariate Gaussian distribution. Hence, for non-Gaussian and/or multi-modal distributed data, this method fails to find anomalies well. Also, instead of a confidence level, the number of outliers to be detected by this method can be controlled by setting a contamination parameter. Be default, this contamination parameter is set to 0.1. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Perform 2D Kernel Density Estimation",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "# Generate the KDE surface as Z\nkde = KernelDensity(kernel='gaussian',bandwidth=0.4).fit(X)\nZp = np.exp(kde.score_samples(XY))\nZp = Zp.reshape(Xp.shape)\n\n# Establish a confidence level of 95% (or 5% cutoff) \n# for the UCL using the quantile of kde_scores.\nscores = kde.score_samples(X)\nthreshold = np.quantile(scores,0.05)\nprint(f\"Threshold (KDE) = {np.exp(threshold)}\")\n\n# Get the anomalous data points\nnormals = X[scores > threshold,:]\nanomals = X[scores <= threshold,:]\n\ncntr = plt.contourf(Xp, Yp, Zp, cmap='viridis')\nplt.scatter(normals[:,0], normals[:,1], s=5, color='k', label='Normal Observations')\nplt.scatter(anomals[:,0], anomals[:,1], s=5, color='r', label='Anomalies (95% Confidence)')\nplt.title('Anomaly Detection using KDE')\nplt.colorbar(cntr)\nplt.legend()\nplt.grid()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In this implementation of KDE for anomaly detection, only the quantile statistic of the KDE scores was used to find anomalies. This approach only gives us an empirical threshold. The actual threshold should be computed by integrating the KDE score surface and finding the threshold $T$ such that $P(x < T) = \\alpha$ for a given confidence level, $\\alpha$. However, numerical double integration of surfaces takes a long time even for our example. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Perform One-class SVM",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ocsvm = OneClassSVM(nu=0.05, gamma=1).fit(X)\nZp = ocsvm.score_samples(XY)\nZp = Zp.reshape(Xp.shape)\n\n# Get the anomalous data points\ny_pred = ocsvm.predict(X)\nnormals = X[y_pred == 1,:]\nanomals = X[y_pred == -1,:]\n\ncntr = plt.contourf(Xp, Yp, Zp, levels=50, cmap='viridis')\nplt.scatter(normals[:,0], normals[:,1], s=5, color='k', label='Normal Observations')\nplt.scatter(anomals[:,0], anomals[:,1], s=5, color='r', label='Anomalies (nu=0.05, gamma=1)')\nplt.title('Anomaly Detection using One-Class SVM')\nplt.colorbar(cntr)\nplt.legend()\nplt.grid()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "OC-SVM is known to be sensitive to outliers that already exist in the training data. If these outliers are treated as \"normal\", or worse, as support vectors, then they will not be flagged as abnormal during testing time. Hence, OC-SVM is only suitable for novelty detection when the training data itself is not contaminated by too many outliers. This method is also efficient when it comes to novelty detection in high-dimensional data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Perform Local Outlier Factor",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lof = LocalOutlierFactor(n_neighbors=5,novelty=True).fit(X)\nZp = lof.score_samples(XY)\nZp = Zp.reshape(Xp.shape)\n\n# Get the anomalous data points\ny_pred = lof.predict(X)\nnormals = X[y_pred == 1,:]\nanomals = X[y_pred == -1,:]\n\ncntr = plt.contourf(Xp, Yp, Zp, levels=10, cmap='viridis')\nplt.scatter(normals[:,0], normals[:,1], s=5, color='k', label='Normal Observations')\nplt.scatter(anomals[:,0], anomals[:,1], s=5, color='r', label='Anomalies (LOF, 5 neighbors)')\nplt.title('Anomaly Detection using LOF')\nplt.colorbar(cntr)\nplt.legend()\nplt.grid()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As seen in the result, the LOF tries to learn the shape of the data set based on the density of points. The less dense the data points, the more likely will they be treated as outliers. However, this also means that if data points are sparsely located **inside** the shape, the LOF will also treat them as outliers.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Perform Isolation Forest",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "isoforest = IsolationForest(contamination=0.1).fit(X)\nZp = isoforest.score_samples(XY)\nZp = Zp.reshape(Xp.shape)\n\n# Get the anomalous data points\ny_pred = isoforest.predict(X)\nnormals = X[y_pred == 1,:]\nanomals = X[y_pred == -1,:]\n\ncntr = plt.contourf(Xp, Yp, Zp, levels=20, cmap='viridis')\nplt.scatter(normals[:,0], normals[:,1], s=5, color='k', label='Normal Observations')\nplt.scatter(anomals[:,0], anomals[:,1], s=5, color='r', label='Anomalies (Contamination = 0.1)')\nplt.title('Anomaly Detection using Isolation Forest')\nplt.colorbar(cntr)\nplt.legend()\nplt.grid()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The Isolation Forest creates grid-like prediction surfaces in the entire space, due to the data splitting nature of the tree ensemble. However, for multi-modal distributed data such as in our example, areas of non-outlier predictions (green) somehow appear at other quadrants where the modes of the data meet, even if no data points are expected to appear there.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}