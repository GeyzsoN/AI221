{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<main style=\"font-family: TeX Gyre Termes; font-size: 1.2rem\">\n",
    "\n",
    "### MEX #7 - Geyzson Kristoffer\n",
    "SN:2023-21036\n",
    "\n",
    "https://uvle.upd.edu.ph/mod/assign/view.php?id=547271\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Polyuria</th>\n",
       "      <th>Polydipsia</th>\n",
       "      <th>sudden weight loss</th>\n",
       "      <th>weakness</th>\n",
       "      <th>Polyphagia</th>\n",
       "      <th>Genital thrush</th>\n",
       "      <th>visual blurring</th>\n",
       "      <th>Itching</th>\n",
       "      <th>Irritability</th>\n",
       "      <th>delayed healing</th>\n",
       "      <th>partial paresis</th>\n",
       "      <th>muscle stiffness</th>\n",
       "      <th>Alopecia</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Gender Polyuria Polydipsia sudden weight loss weakness Polyphagia  \\\n",
       "0   40   Male       No        Yes                 No      Yes         No   \n",
       "1   58   Male       No         No                 No      Yes         No   \n",
       "2   41   Male      Yes         No                 No      Yes        Yes   \n",
       "3   45   Male       No         No                Yes      Yes        Yes   \n",
       "4   60   Male      Yes        Yes                Yes      Yes        Yes   \n",
       "\n",
       "  Genital thrush visual blurring Itching Irritability delayed healing  \\\n",
       "0             No              No     Yes           No             Yes   \n",
       "1             No             Yes      No           No              No   \n",
       "2             No              No     Yes           No             Yes   \n",
       "3            Yes              No     Yes           No             Yes   \n",
       "4             No             Yes     Yes          Yes             Yes   \n",
       "\n",
       "  partial paresis muscle stiffness Alopecia Obesity     class  \n",
       "0              No              Yes      Yes     Yes  Positive  \n",
       "1             Yes               No      Yes      No  Positive  \n",
       "2              No              Yes      Yes      No  Positive  \n",
       "3              No               No       No      No  Positive  \n",
       "4             Yes              Yes      Yes     Yes  Positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('diabetes_data_upload.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 520 entries, 0 to 519\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Age                 520 non-null    int64 \n",
      " 1   Gender              520 non-null    object\n",
      " 2   Polyuria            520 non-null    object\n",
      " 3   Polydipsia          520 non-null    object\n",
      " 4   sudden weight loss  520 non-null    object\n",
      " 5   weakness            520 non-null    object\n",
      " 6   Polyphagia          520 non-null    object\n",
      " 7   Genital thrush      520 non-null    object\n",
      " 8   visual blurring     520 non-null    object\n",
      " 9   Itching             520 non-null    object\n",
      " 10  Irritability        520 non-null    object\n",
      " 11  delayed healing     520 non-null    object\n",
      " 12  partial paresis     520 non-null    object\n",
      " 13  muscle stiffness    520 non-null    object\n",
      " 14  Alopecia            520 non-null    object\n",
      " 15  Obesity             520 non-null    object\n",
      " 16  class               520 non-null    object\n",
      "dtypes: int64(1), object(16)\n",
      "memory usage: 69.2+ KB\n"
     ]
    }
   ],
   "source": [
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>520.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48.028846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.151466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age\n",
       "count  520.000000\n",
       "mean    48.028846\n",
       "std     12.151466\n",
       "min     16.000000\n",
       "25%     39.000000\n",
       "50%     47.500000\n",
       "75%     57.000000\n",
       "max     90.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.describe()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the categorical data to numerical data\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-01-22 19:26:36,249] A new study created in memory with name: no-name-4c669041-1e28-468e-be96-55d83c5f7462\n",
      "[I 2024-01-22 19:26:37,286] Trial 0 finished with value: 0.9019666467344362 and parameters: {'n_estimators': 306, 'max_depth': 24, 'min_samples_split': 10, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.9019666467344362.\n",
      "[I 2024-01-22 19:26:37,779] Trial 1 finished with value: 0.925021593249618 and parameters: {'n_estimators': 295, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.925021593249618.\n",
      "[I 2024-01-22 19:26:38,302] Trial 2 finished with value: 0.9346333576949483 and parameters: {'n_estimators': 370, 'max_depth': 12, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9346333576949483.\n",
      "[I 2024-01-22 19:26:38,829] Trial 3 finished with value: 0.9519301043120058 and parameters: {'n_estimators': 390, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:39,233] Trial 4 finished with value: 0.9461608309525391 and parameters: {'n_estimators': 89, 'max_depth': 14, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:39,891] Trial 5 finished with value: 0.8827652647664607 and parameters: {'n_estimators': 736, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:40,279] Trial 6 finished with value: 0.8904059530928178 and parameters: {'n_estimators': 892, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 13}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:40,687] Trial 7 finished with value: 0.9192412464288088 and parameters: {'n_estimators': 933, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:40,885] Trial 8 finished with value: 0.9500033220384028 and parameters: {'n_estimators': 417, 'max_depth': 27, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:41,215] Trial 9 finished with value: 0.8942705911013665 and parameters: {'n_estimators': 761, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 14}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:41,332] Trial 10 finished with value: 0.9115562642570815 and parameters: {'n_estimators': 11, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:41,584] Trial 11 finished with value: 0.9442229752175936 and parameters: {'n_estimators': 525, 'max_depth': 21, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:41,838] Trial 12 finished with value: 0.9442340486789361 and parameters: {'n_estimators': 529, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:42,038] Trial 13 finished with value: 0.9480765397647998 and parameters: {'n_estimators': 406, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:42,152] Trial 14 finished with value: 0.9500033220384028 and parameters: {'n_estimators': 175, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:42,442] Trial 15 finished with value: 0.925021593249618 and parameters: {'n_estimators': 623, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:42,560] Trial 16 finished with value: 0.9480876132261423 and parameters: {'n_estimators': 216, 'max_depth': 24, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:42,758] Trial 17 finished with value: 0.8981352291099153 and parameters: {'n_estimators': 422, 'max_depth': 3, 'min_samples_split': 12, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:43,066] Trial 18 finished with value: 0.9384537018581268 and parameters: {'n_estimators': 676, 'max_depth': 18, 'min_samples_split': 14, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:43,292] Trial 19 finished with value: 0.9019666467344362 and parameters: {'n_estimators': 482, 'max_depth': 27, 'min_samples_split': 9, 'min_samples_leaf': 11}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:43,567] Trial 20 finished with value: 0.9211569552410692 and parameters: {'n_estimators': 606, 'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9519301043120058.\n",
      "[I 2024-01-22 19:26:43,674] Trial 21 finished with value: 0.9557725953978694 and parameters: {'n_estimators': 172, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.9557725953978694.\n",
      "[I 2024-01-22 19:26:43,784] Trial 22 finished with value: 0.9500143954997453 and parameters: {'n_estimators': 179, 'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.9557725953978694.\n",
      "[I 2024-01-22 19:26:43,868] Trial 23 finished with value: 0.9365269195845237 and parameters: {'n_estimators': 147, 'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 21 with value: 0.9557725953978694.\n",
      "[I 2024-01-22 19:26:44,011] Trial 24 finished with value: 0.9615529422186787 and parameters: {'n_estimators': 273, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 24 with value: 0.9615529422186787.\n",
      "[I 2024-01-22 19:26:44,156] Trial 25 finished with value: 0.940402631054415 and parameters: {'n_estimators': 284, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 24 with value: 0.9615529422186787.\n",
      "[I 2024-01-22 19:26:44,202] Trial 26 finished with value: 0.9615418687573362 and parameters: {'n_estimators': 68, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 24 with value: 0.9615529422186787.\n",
      "[I 2024-01-22 19:26:44,223] Trial 27 finished with value: 0.9307797931477421 and parameters: {'n_estimators': 10, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 24 with value: 0.9615529422186787.\n",
      "[I 2024-01-22 19:26:44,289] Trial 28 finished with value: 0.9115562642570815 and parameters: {'n_estimators': 95, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 24 with value: 0.9615529422186787.\n",
      "[I 2024-01-22 19:26:44,408] Trial 29 finished with value: 0.9269373020618786 and parameters: {'n_estimators': 241, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 24 with value: 0.9615529422186787.\n",
      "[I 2024-01-22 19:26:44,455] Trial 30 finished with value: 0.9654065067658847 and parameters: {'n_estimators': 69, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 30 with value: 0.9654065067658847.\n",
      "[I 2024-01-22 19:26:44,515] Trial 31 finished with value: 0.9596261599450756 and parameters: {'n_estimators': 91, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 30 with value: 0.9654065067658847.\n",
      "[I 2024-01-22 19:26:44,576] Trial 32 finished with value: 0.9615418687573362 and parameters: {'n_estimators': 71, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 30 with value: 0.9654065067658847.\n",
      "[I 2024-01-22 19:26:44,739] Trial 33 finished with value: 0.925021593249618 and parameters: {'n_estimators': 321, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 30 with value: 0.9654065067658847.\n",
      "[I 2024-01-22 19:26:44,786] Trial 34 finished with value: 0.923094810976015 and parameters: {'n_estimators': 59, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 30 with value: 0.9654065067658847.\n",
      "[I 2024-01-22 19:26:44,858] Trial 35 finished with value: 0.9692600713130911 and parameters: {'n_estimators': 127, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:44,991] Trial 36 finished with value: 0.9307687196863995 and parameters: {'n_estimators': 257, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,161] Trial 37 finished with value: 0.9500033220384028 and parameters: {'n_estimators': 332, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,234] Trial 38 finished with value: 0.8807941886474874 and parameters: {'n_estimators': 133, 'max_depth': 1, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,339] Trial 39 finished with value: 0.9192523198901513 and parameters: {'n_estimators': 211, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,374] Trial 40 finished with value: 0.9307908666090846 and parameters: {'n_estimators': 45, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,443] Trial 41 finished with value: 0.9576883042101301 and parameters: {'n_estimators': 109, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,502] Trial 42 finished with value: 0.9692600713130911 and parameters: {'n_estimators': 78, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,582] Trial 43 finished with value: 0.9692489978517486 and parameters: {'n_estimators': 130, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.9692600713130911.\n",
      "[I 2024-01-22 19:26:45,664] Trial 44 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 134, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:26:45,746] Trial 45 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 134, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:26:45,829] Trial 46 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 133, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:26:46,006] Trial 47 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 354, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:26:46,128] Trial 48 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 228, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 44 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:26:46,308] Trial 49 finished with value: 0.9769339800234756 and parameters: {'n_estimators': 347, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:46,476] Trial 50 finished with value: 0.9019555732730936 and parameters: {'n_estimators': 358, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 12}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:46,703] Trial 51 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 477, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:46,941] Trial 52 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 470, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:47,170] Trial 53 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 458, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:47,401] Trial 54 finished with value: 0.9442340486789361 and parameters: {'n_estimators': 480, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:47,657] Trial 55 finished with value: 0.9000287909994906 and parameters: {'n_estimators': 562, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:47,884] Trial 56 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 444, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:48,103] Trial 57 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 450, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:48,377] Trial 58 finished with value: 0.955783668859212 and parameters: {'n_estimators': 558, 'max_depth': 23, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:48,569] Trial 59 finished with value: 0.8904059530928178 and parameters: {'n_estimators': 402, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 14}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:48,783] Trial 60 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 438, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:49,002] Trial 61 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 457, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:49,230] Trial 62 finished with value: 0.9730693420149271 and parameters: {'n_estimators': 436, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:49,470] Trial 63 finished with value: 0.953856886585609 and parameters: {'n_estimators': 494, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:49,689] Trial 64 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 457, 'max_depth': 22, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:49,923] Trial 65 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 382, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:50,200] Trial 66 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 526, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:50,491] Trial 67 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 558, 'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:50,810] Trial 68 finished with value: 0.9461497574911966 and parameters: {'n_estimators': 602, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:51,284] Trial 69 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 996, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:51,603] Trial 70 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 695, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:51,828] Trial 71 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 469, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:52,074] Trial 72 finished with value: 0.9596261599450756 and parameters: {'n_estimators': 522, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:52,282] Trial 73 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 429, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:52,474] Trial 74 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 396, 'max_depth': 22, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:52,716] Trial 75 finished with value: 0.9615418687573362 and parameters: {'n_estimators': 508, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:53,013] Trial 76 finished with value: 0.9480765397647998 and parameters: {'n_estimators': 641, 'max_depth': 21, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:53,168] Trial 77 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 301, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:53,424] Trial 78 finished with value: 0.9538568865856089 and parameters: {'n_estimators': 529, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:53,803] Trial 79 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 803, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:54,073] Trial 80 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 580, 'max_depth': 26, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:54,312] Trial 81 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 456, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:54,545] Trial 82 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 475, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:54,756] Trial 83 finished with value: 0.9711536332026665 and parameters: {'n_estimators': 422, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:54,928] Trial 84 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 335, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:55,137] Trial 85 finished with value: 0.971164706664009 and parameters: {'n_estimators': 370, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:55,407] Trial 86 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 537, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:55,646] Trial 87 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 470, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:55,904] Trial 88 finished with value: 0.9711536332026665 and parameters: {'n_estimators': 501, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:56,113] Trial 89 finished with value: 0.9019444998117511 and parameters: {'n_estimators': 436, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:56,306] Trial 90 finished with value: 0.9000287909994906 and parameters: {'n_estimators': 400, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 11}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:56,527] Trial 91 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 436, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:56,736] Trial 92 finished with value: 0.9769229065621333 and parameters: {'n_estimators': 421, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:56,978] Trial 93 finished with value: 0.9769229065621333 and parameters: {'n_estimators': 503, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:57,228] Trial 94 finished with value: 0.9692268509290635 and parameters: {'n_estimators': 505, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:57,403] Trial 95 finished with value: 0.9577104511328152 and parameters: {'n_estimators': 352, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:57,594] Trial 96 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 383, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:57,799] Trial 97 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 414, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:58,056] Trial 98 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 535, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 49 with value: 0.9769339800234756.\n",
      "[I 2024-01-22 19:26:58,320] Trial 99 finished with value: 0.8942595176400238 and parameters: {'n_estimators': 595, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 13}. Best is trial 49 with value: 0.9769339800234756.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  100\n",
      "Best trial:\n",
      "Value: 0.9769339800234756\n",
      "Params: \n",
      "    n_estimators: 347\n",
      "    max_depth: 21\n",
      "    min_samples_split: 2\n",
      "    min_samples_leaf: 1\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data\n",
    "data = diabetes.copy()\n",
    "\n",
    "# Assuming the last column is the target and the rest are features\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X = X.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 30)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 14)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 14)\n",
    "\n",
    "    # Model to be trained\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "    )\n",
    "\n",
    "    # Scoring method\n",
    "    score = cross_val_score(classifier, X, y, n_jobs=-1, cv=3)\n",
    "    accuracy = score.mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object and specify the optimization direction to 'maximize' accuracy\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the study, the objective function is passed in as the first argument\n",
    "study.optimize(objective, n_trials=100)  # Specify the number of trials\n",
    "\n",
    "# Output the best parameters\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value: {}'.format(trial.value))\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-22 19:32:56,090] A new study created in memory with name: no-name-f09a6c84-f80e-4809-bc3e-6c919be1658c\n",
      "[I 2024-01-22 19:32:57,325] Trial 0 finished with value: 0.8961862999136271 and parameters: {'n_estimators': 665, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8961862999136271.\n",
      "[I 2024-01-22 19:32:57,785] Trial 1 finished with value: 0.9192633933514939 and parameters: {'n_estimators': 212, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9192633933514939.\n",
      "[I 2024-01-22 19:32:58,580] Trial 2 finished with value: 0.971164706664009 and parameters: {'n_estimators': 902, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:32:58,807] Trial 3 finished with value: 0.8942484441786814 and parameters: {'n_estimators': 511, 'max_depth': 1, 'min_samples_split': 4, 'min_samples_leaf': 13}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:32:59,101] Trial 4 finished with value: 0.8942595176400238 and parameters: {'n_estimators': 691, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 12}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:32:59,259] Trial 5 finished with value: 0.8865745354682968 and parameters: {'n_estimators': 355, 'max_depth': 10, 'min_samples_split': 14, 'min_samples_leaf': 14}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:32:59,514] Trial 6 finished with value: 0.8981020087258876 and parameters: {'n_estimators': 582, 'max_depth': 28, 'min_samples_split': 4, 'min_samples_leaf': 11}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:32:59,990] Trial 7 finished with value: 0.8981130821872302 and parameters: {'n_estimators': 290, 'max_depth': 11, 'min_samples_split': 12, 'min_samples_leaf': 12}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:00,570] Trial 8 finished with value: 0.9000287909994906 and parameters: {'n_estimators': 534, 'max_depth': 14, 'min_samples_split': 12, 'min_samples_leaf': 9}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:00,730] Trial 9 finished with value: 0.8789006267579119 and parameters: {'n_estimators': 356, 'max_depth': 2, 'min_samples_split': 13, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:01,180] Trial 10 finished with value: 0.9615529422186787 and parameters: {'n_estimators': 995, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:01,637] Trial 11 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 984, 'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:02,091] Trial 12 finished with value: 0.9634797244922817 and parameters: {'n_estimators': 1000, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:02,433] Trial 13 finished with value: 0.9269373020618786 and parameters: {'n_estimators': 757, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:02,802] Trial 14 finished with value: 0.9480876132261423 and parameters: {'n_estimators': 800, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:03,194] Trial 15 finished with value: 0.9327176488826877 and parameters: {'n_estimators': 869, 'max_depth': 17, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:03,587] Trial 16 finished with value: 0.9230948109760148 and parameters: {'n_estimators': 887, 'max_depth': 24, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:03,970] Trial 17 finished with value: 0.9307908666090846 and parameters: {'n_estimators': 858, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:04,002] Trial 18 finished with value: 0.944211901756251 and parameters: {'n_estimators': 34, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:04,426] Trial 19 finished with value: 0.9038823555466968 and parameters: {'n_estimators': 939, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:04,726] Trial 20 finished with value: 0.9461608309525391 and parameters: {'n_estimators': 661, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:05,163] Trial 21 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 961, 'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:05,510] Trial 22 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 760, 'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:05,928] Trial 23 finished with value: 0.9519301043120058 and parameters: {'n_estimators': 935, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:06,321] Trial 24 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 838, 'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:06,753] Trial 25 finished with value: 0.925021593249618 and parameters: {'n_estimators': 940, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:07,083] Trial 26 finished with value: 0.9557725953978694 and parameters: {'n_estimators': 727, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:07,527] Trial 27 finished with value: 0.9269373020618786 and parameters: {'n_estimators': 992, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:07,901] Trial 28 finished with value: 0.9538458131242664 and parameters: {'n_estimators': 807, 'max_depth': 21, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:08,182] Trial 29 finished with value: 0.9288530108741391 and parameters: {'n_estimators': 600, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:08,586] Trial 30 finished with value: 0.9173255376165482 and parameters: {'n_estimators': 907, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:08,951] Trial 31 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 792, 'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.971164706664009.\n",
      "[I 2024-01-22 19:33:09,257] Trial 32 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 653, 'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:09,482] Trial 33 finished with value: 0.9480765397647998 and parameters: {'n_estimators': 455, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:09,791] Trial 34 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 646, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:09,999] Trial 35 finished with value: 0.8942705911013665 and parameters: {'n_estimators': 452, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:10,058] Trial 36 finished with value: 0.9519190308506632 and parameters: {'n_estimators': 91, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:10,389] Trial 37 finished with value: 0.9634797244922817 and parameters: {'n_estimators': 703, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:10,811] Trial 38 finished with value: 0.9365712134298939 and parameters: {'n_estimators': 948, 'max_depth': 29, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:11,178] Trial 39 finished with value: 0.9000287909994906 and parameters: {'n_estimators': 848, 'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:11,440] Trial 40 finished with value: 0.8904170265541603 and parameters: {'n_estimators': 603, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 14}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:11,784] Trial 41 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 750, 'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:12,204] Trial 42 finished with value: 0.9692268509290635 and parameters: {'n_estimators': 900, 'max_depth': 21, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:12,607] Trial 43 finished with value: 0.9576993776714726 and parameters: {'n_estimators': 898, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:13,050] Trial 44 finished with value: 0.9615418687573362 and parameters: {'n_estimators': 998, 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:13,423] Trial 45 finished with value: 0.9519190308506632 and parameters: {'n_estimators': 815, 'max_depth': 21, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:13,843] Trial 46 finished with value: 0.9461497574911966 and parameters: {'n_estimators': 907, 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:13,953] Trial 47 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 211, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:14,385] Trial 48 finished with value: 0.9615529422186787 and parameters: {'n_estimators': 966, 'max_depth': 26, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:14,786] Trial 49 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 877, 'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:15,183] Trial 50 finished with value: 0.9384869222421545 and parameters: {'n_estimators': 867, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 32 with value: 0.9730804154762694.\n",
      "[I 2024-01-22 19:33:15,597] Trial 51 finished with value: 0.9749961242885301 and parameters: {'n_estimators': 914, 'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:15,963] Trial 52 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 783, 'max_depth': 24, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:16,378] Trial 53 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 902, 'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:16,745] Trial 54 finished with value: 0.8904059530928178 and parameters: {'n_estimators': 843, 'max_depth': 23, 'min_samples_split': 3, 'min_samples_leaf': 13}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:17,174] Trial 55 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 915, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:17,427] Trial 56 finished with value: 0.9557725953978694 and parameters: {'n_estimators': 534, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:17,718] Trial 57 finished with value: 0.8962195202976547 and parameters: {'n_estimators': 689, 'max_depth': 1, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:18,106] Trial 58 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 835, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:18,538] Trial 59 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 821, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:18,864] Trial 60 finished with value: 0.9038823555466968 and parameters: {'n_estimators': 733, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:19,253] Trial 61 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 830, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 51 with value: 0.9749961242885301.\n",
      "[I 2024-01-22 19:33:19,683] Trial 62 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 926, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:20,109] Trial 63 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 928, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:20,523] Trial 64 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 879, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:20,886] Trial 65 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 774, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:21,329] Trial 66 finished with value: 0.955783668859212 and parameters: {'n_estimators': 963, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:21,731] Trial 67 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 856, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:22,149] Trial 68 finished with value: 0.9230948109760148 and parameters: {'n_estimators': 927, 'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:22,555] Trial 69 finished with value: 0.9500033220384028 and parameters: {'n_estimators': 872, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:22,998] Trial 70 finished with value: 0.9692268509290635 and parameters: {'n_estimators': 967, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:23,383] Trial 71 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 811, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:23,811] Trial 72 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 914, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:24,225] Trial 73 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 888, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:24,651] Trial 74 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 894, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:25,089] Trial 75 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 945, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:25,524] Trial 76 finished with value: 0.9461497574911967 and parameters: {'n_estimators': 977, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:25,949] Trial 77 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 934, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:26,139] Trial 78 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 378, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:26,533] Trial 79 finished with value: 0.9384647753194694 and parameters: {'n_estimators': 882, 'max_depth': 24, 'min_samples_split': 11, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:26,995] Trial 80 finished with value: 0.9500033220384028 and parameters: {'n_estimators': 1000, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:27,399] Trial 81 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 853, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:27,841] Trial 82 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 943, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:28,272] Trial 83 finished with value: 0.9711536332026665 and parameters: {'n_estimators': 913, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:28,642] Trial 84 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 788, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:29,082] Trial 85 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 955, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:29,445] Trial 86 finished with value: 0.8981020087258876 and parameters: {'n_estimators': 830, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 11}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:29,859] Trial 87 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 885, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:30,260] Trial 88 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 883, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:30,563] Trial 89 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 624, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:30,992] Trial 90 finished with value: 0.9673111421168029 and parameters: {'n_estimators': 920, 'max_depth': 26, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:31,396] Trial 91 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 856, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:31,854] Trial 92 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 976, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:32,116] Trial 93 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 553, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:32,370] Trial 94 finished with value: 0.9653954333045424 and parameters: {'n_estimators': 538, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:32,625] Trial 95 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 465, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:32,868] Trial 96 finished with value: 0.9634575775695967 and parameters: {'n_estimators': 510, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:33,076] Trial 97 finished with value: 0.9730804154762694 and parameters: {'n_estimators': 403, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:33,344] Trial 98 finished with value: 0.9519301043120058 and parameters: {'n_estimators': 579, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 62 with value: 0.9750071977498727.\n",
      "[I 2024-01-22 19:33:33,628] Trial 99 finished with value: 0.9750071977498727 and parameters: {'n_estimators': 578, 'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.9750071977498727.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  100\n",
      "Best trial:\n",
      "Value: 0.9750071977498727\n",
      "Params: \n",
      "    n_estimators: 926\n",
      "    max_depth: 16\n",
      "    min_samples_split: 3\n",
      "    min_samples_leaf: 1\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data\n",
    "data = diabetes.copy()\n",
    "\n",
    "# Encode the target variable\n",
    "data['class'] = data['class'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "# Assuming the last column is the target and the rest are features\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X = X.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 30)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 14)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 14)\n",
    "\n",
    "    # Model to be trained\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "    )\n",
    "\n",
    "    # Scoring method\n",
    "    score = cross_val_score(classifier, X, y, n_jobs=-1, cv=3)\n",
    "    accuracy = score.mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object and specify the optimization direction to 'maximize' accuracy\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the study, the objective function is passed in as the first argument\n",
    "study.optimize(objective, n_trials=100)  # Specify the number of trials\n",
    "\n",
    "# Output the best parameters\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value: {}'.format(trial.value))\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-23 01:01:32,547] A new study created in memory with name: no-name-5ff52b22-adcf-4fb1-8baa-bde6015c5e42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-23 01:01:32,590] Trial 0 finished with value: 0.9685529076293784 and parameters: {'classifier': 'SVM', 'C': 64.20514710420048, 'kernel': 'poly', 'degree': 5, 'gamma': 'auto'}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:33,002] Trial 1 finished with value: 0.9592310041060831 and parameters: {'classifier': 'MLPClassifier', 'hidden_layer_sizes': '(50, 100, 50)', 'activation': 'relu', 'solver': 'adam', 'max_iter': 1023, 'batch_size': 29, 'learning_rate_init': 0.009546165503450057, 'alpha': 0.006867118928118159, 'shuffle': True, 'tol': 0.007896427932554134, 'momentum': 0.0007097895955318593, 'early_stopping': False}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:33,530] Trial 2 finished with value: 0.9184858659020962 and parameters: {'classifier': 'SVM', 'C': 90.5519811367174, 'kernel': 'linear', 'degree': 6, 'gamma': 'auto'}. Best is trial 0 with value: 0.9685529076293784.\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2024-01-23 01:01:33,557] Trial 3 finished with value: 0.9206750249827091 and parameters: {'classifier': 'LogisticRegression', 'C': 4.062088825493547, 'penalty': 'none', 'max_iter': 1760}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:33,585] Trial 4 finished with value: 0.869487135401666 and parameters: {'classifier': 'SVM', 'C': 10.42363907130163, 'kernel': 'sigmoid', 'degree': 9, 'gamma': 'auto'}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:34,055] Trial 5 finished with value: 0.959314505479607 and parameters: {'classifier': 'MLPClassifier', 'hidden_layer_sizes': '(100,)', 'activation': 'relu', 'solver': 'adam', 'max_iter': 1671, 'batch_size': 8, 'learning_rate_init': 0.009489250061854183, 'alpha': 0.003277677155058812, 'shuffle': True, 'tol': 0.009725891614062251, 'momentum': 0.00948635645762207, 'early_stopping': False}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:34,071] Trial 6 finished with value: 0.8937987440656142 and parameters: {'classifier': 'NaiveBayes', 'var_smoothing': 0.004938216657938123}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:34,281] Trial 7 finished with value: 0.8900956606787741 and parameters: {'classifier': 'MLPClassifier', 'hidden_layer_sizes': '(100,)', 'activation': 'tanh', 'solver': 'sgd', 'max_iter': 1821, 'batch_size': 76, 'learning_rate_init': 0.005814926006745984, 'alpha': 0.0006106457024951537, 'shuffle': True, 'tol': 0.0025550908516353958, 'momentum': 9.932310887301862e-05, 'early_stopping': False}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:34,309] Trial 8 finished with value: 0.8937987440656142 and parameters: {'classifier': 'NaiveBayes', 'var_smoothing': 0.0007481004122710007}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:34,356] Trial 9 finished with value: 0.941082687288192 and parameters: {'classifier': 'XGBoost', 'n_estimators': 46, 'max_depth': 47, 'learning_rate': 0.7575150337676043, 'booster': 'gbtree', 'reg_alpha': 0.963073889053621, 'reg_lambda': 0.1945559103695241, 'min_child_weight': 4, 'scale_pos_weight': 748042.3173912914}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:34,390] Trial 10 finished with value: 0.947436251477441 and parameters: {'classifier': 'kNN', 'n_neighbors': 15, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 32}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:36,193] Trial 11 finished with value: 0.8866197096892791 and parameters: {'classifier': 'RandomForest', 'n_estimators': 962, 'max_depth': 1, 'criterion': 'gini', 'min_samples_split': 5, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:36,240] Trial 12 finished with value: 0.9184858659020962 and parameters: {'classifier': 'SVM', 'C': 70.52679026815937, 'kernel': 'poly', 'degree': 1, 'gamma': 'scale'}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:36,500] Trial 13 finished with value: 0.9137347104859215 and parameters: {'classifier': 'MLPClassifier', 'hidden_layer_sizes': '(100,)', 'activation': 'relu', 'solver': 'adam', 'max_iter': 1369, 'batch_size': 7, 'learning_rate_init': 0.0003757822742967822, 'alpha': 0.0028241905638487063, 'shuffle': False, 'tol': 0.009925945058849063, 'momentum': 0.009717422701693141, 'early_stopping': True}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:36,537] Trial 14 finished with value: 0.9206750249827091 and parameters: {'classifier': 'LogisticRegression', 'C': 47.69955253144818, 'penalty': 'l2', 'max_iter': 1497}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:37,980] Trial 15 finished with value: 0.9489929905579654 and parameters: {'classifier': 'RandomForest', 'n_estimators': 535, 'max_depth': 19, 'criterion': 'entropy', 'min_samples_split': 14, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 0 with value: 0.9685529076293784.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [01:01:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_depth\", \"min_child_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-01-23 01:01:38,023] Trial 16 finished with value: 0.4483324684052751 and parameters: {'classifier': 'XGBoost', 'n_estimators': 45, 'max_depth': 50, 'learning_rate': 0.028046518565744305, 'booster': 'gblinear', 'reg_alpha': 0.1251484603936034, 'reg_lambda': 0.981375059690235, 'min_child_weight': 10, 'scale_pos_weight': 7581.872073525796}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:38,056] Trial 17 finished with value: 0.858848153071491 and parameters: {'classifier': 'kNN', 'n_neighbors': 50, 'weights': 'uniform', 'algorithm': 'kd_tree', 'leaf_size': 2}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:38,090] Trial 18 finished with value: 0.9685299037414559 and parameters: {'classifier': 'SVM', 'C': 46.01074082434051, 'kernel': 'poly', 'degree': 3, 'gamma': 'auto'}. Best is trial 0 with value: 0.9685529076293784.\n",
      "[I 2024-01-23 01:01:38,123] Trial 19 finished with value: 0.9685299037414559 and parameters: {'classifier': 'SVM', 'C': 46.71521243346855, 'kernel': 'poly', 'degree': 3, 'gamma': 'auto'}. Best is trial 0 with value: 0.9685529076293784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial final score: 0.9685529076293784\n",
      "classifier: SVM\n",
      "C: 64.20514710420048\n",
      "kernel: poly\n",
      "degree: 5\n",
      "gamma: auto\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load your data\n",
    "df = diabetes.copy()\n",
    "\n",
    "# Assuming 'class' is the target variable\n",
    "df['class'] = df['class'].map({'Positive': 1, 'Negative': 0})\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X = X.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features (important for models like SVM, kNN, and MLP)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['MLPClassifier', 'RandomForest', 'XGBoost', 'LogisticRegression', 'NaiveBayes', 'SVM', 'kNN'])\n",
    "    \n",
    "    # Hyperparameters spaces for each model\n",
    "    if classifier_name == 'MLPClassifier':\n",
    "        hidden_layer_sizes_str = trial.suggest_categorical('hidden_layer_sizes', ['(50, 50, 50)', '(50, 100, 50)', '(100,)'])\n",
    "        hidden_layer_sizes = eval(hidden_layer_sizes_str)\n",
    "        params = {\n",
    "            'hidden_layer_sizes': hidden_layer_sizes,\n",
    "            'activation': trial.suggest_categorical('activation', ['tanh', 'relu']),\n",
    "            'solver': trial.suggest_categorical('solver', ['sgd', 'adam']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1000, 2000), \n",
    "            'batch_size': trial.suggest_int('batch_size', 1, 100),\n",
    "            'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-5, 1e-2),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-5, 1e-2),\n",
    "            'shuffle': trial.suggest_categorical('shuffle', [True, False]),\n",
    "            'tol': trial.suggest_float('tol', 1e-5, 1e-2),\n",
    "            'momentum': trial.suggest_float('momentum', 1e-5, 1e-2),\n",
    "            'early_stopping': trial.suggest_categorical('early_stopping', [True, False]),\n",
    "        }\n",
    "        model = MLPClassifier(**params)\n",
    "        \n",
    "    elif classifier_name == 'RandomForest':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 50),\n",
    "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 14),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 14),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "        \n",
    "    elif classifier_name == 'XGBoost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0),\n",
    "            'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1e-6, 1e6),\n",
    "\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        \n",
    "    elif classifier_name == 'LogisticRegression':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C', 1e-8, 1e2),\n",
    "            'penalty': trial.suggest_categorical('penalty', ['none', 'l2']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1000, 2000),\n",
    "        }\n",
    "        model = LogisticRegression(**params)\n",
    "        \n",
    "    elif classifier_name == 'NaiveBayes':\n",
    "        params = {\n",
    "            'var_smoothing': trial.suggest_float('var_smoothing', 1e-8, 1e-2),\n",
    "        }\n",
    "        model = GaussianNB(**params)\n",
    "        \n",
    "    elif classifier_name == 'SVM':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C', 1e-8, 1e2),\n",
    "            'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            'degree': trial.suggest_int('degree', 1, 10),\n",
    "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        }\n",
    "        model = SVC(**params)\n",
    "        \n",
    "    elif classifier_name == 'kNN':\n",
    "        params = {\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 1, 50),\n",
    "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "            'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "            'leaf_size': trial.suggest_int('leaf_size', 1, 50),\n",
    "        }\n",
    "        model = KNeighborsClassifier(**params)\n",
    "\n",
    "    f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "    precision_scorer = make_scorer(precision_score)\n",
    "    recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "    # 10-fold cross-validation\n",
    "    score = cross_val_score(model, X_train, y_train, n_jobs=-1, cv=10, scoring=f1_scorer).mean()\n",
    "    return score\n",
    "\n",
    "# Create a study object and specify the optimization direction\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial final score: {best_trial.value}\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Train the best model on the full training data and evaluate it on the test data\n",
    "best_classifier = best_trial.params['classifier']\n",
    "# Remove the classifier name and create a model instance with the rest of the parameters\n",
    "del best_trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.166"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((72+83)/200)*50 + 0.9*30 + 0.8208*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation Metrics:\n",
      "Accuracy: 0.9615384615384616\n",
      "F1 Score: 0.9620799490364706\n",
      "Precision: 0.9656964656964657\n",
      "Recall: 0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Create the best model\n",
    "if best_params['classifier'] == 'MLPClassifier':\n",
    "    model = MLPClassifier(**{k: v for k, v in best_params.items() if k != 'classifier'})\n",
    "elif best_params['classifier'] == 'RandomForest':\n",
    "    model = RandomForestClassifier(**{k: v for k, v in best_params.items() if k != 'classifier'})\n",
    "elif best_params['classifier'] == 'XGBoost':\n",
    "    model = xgb.XGBClassifier(**{k: v for k, v in best_params.items() if k != 'classifier'})\n",
    "elif best_params['classifier'] == 'LogisticRegression':\n",
    "    model = LogisticRegression(**{k: v for k, v in best_params.items() if k != 'classifier'})\n",
    "elif best_params['classifier'] == 'NaiveBayes':\n",
    "    model = GaussianNB()\n",
    "elif best_params['classifier'] == 'SVM':\n",
    "    model = SVC(**{k: v for k, v in best_params.items() if k != 'classifier'})\n",
    "elif best_params['classifier'] == 'kNN':\n",
    "    model = KNeighborsClassifier(**{k: v for k, v in best_params.items() if k != 'classifier'})\n",
    "\n",
    "# Train the model on the full training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Set Evaluation Metrics:\\nAccuracy: {accuracy}\\nF1 Score: {f1}\\nPrecision: {precision}\\nRecall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.05000000000001"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.40*(np.mean([97,85,88,88]))+0.25*(93)+0.25*(92)+0.10*(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Student Number\tFinal Grade\tWgtd Ave\tPA1\tPA2\tPA3\tPA4\tReporting\tExam\tMini-Project\n",
    "XXXX-01404\t1.75\t84.075\t55\t90\t88\t85\t100\t79.1\t90\n",
    "XXXX-04638\t1.5\t85.5\t50\t90\t90\t88\t100\t80.8\t94\n",
    "XXXX-06706\t2.25\t74.35\t55\t75\t88\t88\t100\t55\t80\n",
    "XXXX-07128\t1.0\t92.6\t93\t95\t95\t95\t100\t84.2\t95\n",
    "XXXX-09114\t1.5\t87\t95\t88\t95\t90\t100\t65.8\t95\n",
    "XXXX-10236\t2.75\t66.4\t55\t55\t55\t55\t100\t46.6\t91\n",
    "XXXX-10667\t1.25\t88.025\t60\t88\t88\t90\t100\t91.7\t90\n",
    "XXXX-12034\t3.0\t64.375\t90\t85\t0\t0\t100\t52.5\t95\n",
    "XXXX-19855\t1.5\t86.1\t93\t90\t88\t88\t100\t70.8\t90\n",
    "XXXX-20754\t1.0\t92.1\t88\t88\t90\t85\t100\t92\t96\n",
    "XXXX-20911\t1.25\t90.05\t95\t88\t95\t95\t100\t75\t96\n",
    "XXXX-21036\t1.25\t90.675\t97\t85\t88\t88\t100\t87.5\t92\n",
    "XXXX-21122\t5.0\t39.2\t0\t0\t0\t0\t100\t25.8\t91\n",
    "XXXX-21123\t3.0\t64.1\t60\t50\t40\t50\t100\t51.4\t85\n",
    "XXXX-21128\t2.5\t72.55\t55\t90\t50\t93\t100\t55\t80\n",
    "XXXX-21426\t1.0\t92.175\t88\t90\t85\t83\t100\t98.3\t92\n",
    "XXXX-22085\t1.0\t92.1\t95\t88\t95\t93\t100\t85\t95\n",
    "XXXX-23332\t2.0\t80.075\t55\t88\t75\t85\t100\t69.1\t90\n",
    "XXXX-26390\t5.0\t27.3\t75\t0\t0\t0\t100\t39.2\t0\n",
    "XXXX-28026\t1.0\t96.325\t95\t95\t95\t93\t100\t104.1\t90\n",
    "XXXX-32614\t1.5\t86.975\t93\t93\t95\t93\t100\t73.3\t85\n",
    "XXXX-36198\t5.0\t32.275\t50\t50\t0\t0\t100\t49.1\t0\n",
    "XXXX-46106\t5.0\t38.625\t60\t45\t0\t0\t100\t72.5\t0\n",
    "XXXX-52380\t2.0\t81.35\t95\t88\t0\t90\t100\t84.2\t92\n",
    "XXXX-60910\t2.25\t75.425\t55\t75\t85\t83\t100\t52.5\t90\n",
    "XXXX-62543\t5.0\t37.75\t50\t88\t0\t0\t100\t55.8\t0\n",
    "XXXX-67839\t5.0\t28.55\t0\t0\t0\t0\t100\t74.2\t0\n",
    "XXXX-74467\t1.0\t92.225\t93\t93\t90\t90\t100\t87.5\t95\n",
    "XXXX-78356\t2.5\t70.3\t50\t70\t0\t88\t100\t65\t93\n",
    "XXXX-90339\t5.0\t58.45\t40\t20\t10\t10\t100\t70.8\t91\n",
    "XXXX-98340\t5.0\t53.425\t95\t93\t0\t88\t100\t63.3\t0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student Number</th>\n",
       "      <th>Final Grade</th>\n",
       "      <th>Wgtd Ave</th>\n",
       "      <th>PA1</th>\n",
       "      <th>PA2</th>\n",
       "      <th>PA3</th>\n",
       "      <th>PA4</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Exam</th>\n",
       "      <th>Mini-Project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XXXX-01404</td>\n",
       "      <td>1.75</td>\n",
       "      <td>84.075</td>\n",
       "      <td>55</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>79.1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XXXX-04638</td>\n",
       "      <td>1.5</td>\n",
       "      <td>85.5</td>\n",
       "      <td>50</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>80.8</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XXXX-06706</td>\n",
       "      <td>2.25</td>\n",
       "      <td>74.35</td>\n",
       "      <td>55</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XXXX-07128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.6</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>84.2</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XXXX-09114</td>\n",
       "      <td>1.5</td>\n",
       "      <td>87</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>65.8</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XXXX-10236</td>\n",
       "      <td>2.75</td>\n",
       "      <td>66.4</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>100</td>\n",
       "      <td>46.6</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XXXX-10667</td>\n",
       "      <td>1.25</td>\n",
       "      <td>88.025</td>\n",
       "      <td>60</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>91.7</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XXXX-12034</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.375</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>52.5</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XXXX-19855</td>\n",
       "      <td>1.5</td>\n",
       "      <td>86.1</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>70.8</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XXXX-20754</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.1</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XXXX-20911</td>\n",
       "      <td>1.25</td>\n",
       "      <td>90.05</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XXXX-21036</td>\n",
       "      <td>1.25</td>\n",
       "      <td>90.675</td>\n",
       "      <td>97</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XXXX-21122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>25.8</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XXXX-21123</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>51.4</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XXXX-21128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>72.55</td>\n",
       "      <td>55</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XXXX-21426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.175</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>98.3</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XXXX-22085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.1</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>85</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XXXX-23332</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.075</td>\n",
       "      <td>55</td>\n",
       "      <td>88</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>69.1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>XXXX-26390</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.3</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XXXX-28026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.325</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>104.1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>XXXX-32614</td>\n",
       "      <td>1.5</td>\n",
       "      <td>86.975</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>73.3</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XXXX-36198</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.275</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>49.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>XXXX-46106</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.625</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>72.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XXXX-52380</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.35</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>84.2</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>XXXX-60910</td>\n",
       "      <td>2.25</td>\n",
       "      <td>75.425</td>\n",
       "      <td>55</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>52.5</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XXXX-62543</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.75</td>\n",
       "      <td>50</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>55.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XXXX-67839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>74.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XXXX-74467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.225</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>XXXX-78356</td>\n",
       "      <td>2.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>50</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>65</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>XXXX-90339</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58.45</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>70.8</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>XXXX-98340</td>\n",
       "      <td>5.0</td>\n",
       "      <td>53.425</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>63.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student Number Final Grade Wgtd Ave PA1 PA2 PA3 PA4 Reporting   Exam  \\\n",
       "0      XXXX-01404        1.75   84.075  55  90  88  85       100   79.1   \n",
       "1      XXXX-04638         1.5     85.5  50  90  90  88       100   80.8   \n",
       "2      XXXX-06706        2.25    74.35  55  75  88  88       100     55   \n",
       "3      XXXX-07128         1.0     92.6  93  95  95  95       100   84.2   \n",
       "4      XXXX-09114         1.5       87  95  88  95  90       100   65.8   \n",
       "5      XXXX-10236        2.75     66.4  55  55  55  55       100   46.6   \n",
       "6      XXXX-10667        1.25   88.025  60  88  88  90       100   91.7   \n",
       "7      XXXX-12034         3.0   64.375  90  85   0   0       100   52.5   \n",
       "8      XXXX-19855         1.5     86.1  93  90  88  88       100   70.8   \n",
       "9      XXXX-20754         1.0     92.1  88  88  90  85       100     92   \n",
       "10     XXXX-20911        1.25    90.05  95  88  95  95       100     75   \n",
       "11     XXXX-21036        1.25   90.675  97  85  88  88       100   87.5   \n",
       "12     XXXX-21122         5.0     39.2   0   0   0   0       100   25.8   \n",
       "13     XXXX-21123         3.0     64.1  60  50  40  50       100   51.4   \n",
       "14     XXXX-21128         2.5    72.55  55  90  50  93       100     55   \n",
       "15     XXXX-21426         1.0   92.175  88  90  85  83       100   98.3   \n",
       "16     XXXX-22085         1.0     92.1  95  88  95  93       100     85   \n",
       "17     XXXX-23332         2.0   80.075  55  88  75  85       100   69.1   \n",
       "18     XXXX-26390         5.0     27.3  75   0   0   0       100   39.2   \n",
       "19     XXXX-28026         1.0   96.325  95  95  95  93       100  104.1   \n",
       "20     XXXX-32614         1.5   86.975  93  93  95  93       100   73.3   \n",
       "21     XXXX-36198         5.0   32.275  50  50   0   0       100   49.1   \n",
       "22     XXXX-46106         5.0   38.625  60  45   0   0       100   72.5   \n",
       "23     XXXX-52380         2.0    81.35  95  88   0  90       100   84.2   \n",
       "24     XXXX-60910        2.25   75.425  55  75  85  83       100   52.5   \n",
       "25     XXXX-62543         5.0    37.75  50  88   0   0       100   55.8   \n",
       "26     XXXX-67839         5.0    28.55   0   0   0   0       100   74.2   \n",
       "27     XXXX-74467         1.0   92.225  93  93  90  90       100   87.5   \n",
       "28     XXXX-78356         2.5     70.3  50  70   0  88       100     65   \n",
       "29     XXXX-90339         5.0    58.45  40  20  10  10       100   70.8   \n",
       "30     XXXX-98340         5.0   53.425  95  93   0  88       100   63.3   \n",
       "\n",
       "   Mini-Project  \n",
       "0            90  \n",
       "1            94  \n",
       "2            80  \n",
       "3            95  \n",
       "4            95  \n",
       "5            91  \n",
       "6            90  \n",
       "7            95  \n",
       "8            90  \n",
       "9            96  \n",
       "10           96  \n",
       "11           92  \n",
       "12           91  \n",
       "13           85  \n",
       "14           80  \n",
       "15           92  \n",
       "16           95  \n",
       "17           90  \n",
       "18            0  \n",
       "19           90  \n",
       "20           85  \n",
       "21            0  \n",
       "22            0  \n",
       "23           92  \n",
       "24           90  \n",
       "25            0  \n",
       "26            0  \n",
       "27           95  \n",
       "28           93  \n",
       "29           91  \n",
       "30            0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = text.split('\\n')\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    data.append(row.split('\\t'))\n",
    "\n",
    "data = data[1:-1]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Student Number', 'Final Grade', 'Wgtd Ave', 'PA1', 'PA2', 'PA3', 'PA4', 'Reporting', 'Exam', 'Mini-Project'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student Number</th>\n",
       "      <th>Final Grade</th>\n",
       "      <th>Wgtd Ave</th>\n",
       "      <th>PA1</th>\n",
       "      <th>PA2</th>\n",
       "      <th>PA3</th>\n",
       "      <th>PA4</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Exam</th>\n",
       "      <th>Mini-Project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XXXX-28026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.325</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>104.1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XXXX-07128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.6</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>84.2</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XXXX-74467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.225</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XXXX-21426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.175</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>98.3</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XXXX-20754</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.1</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XXXX-22085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.1</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>85</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XXXX-21036</td>\n",
       "      <td>1.25</td>\n",
       "      <td>90.675</td>\n",
       "      <td>97</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XXXX-20911</td>\n",
       "      <td>1.25</td>\n",
       "      <td>90.05</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XXXX-10667</td>\n",
       "      <td>1.25</td>\n",
       "      <td>88.025</td>\n",
       "      <td>60</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>91.7</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XXXX-09114</td>\n",
       "      <td>1.5</td>\n",
       "      <td>87</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>65.8</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>XXXX-32614</td>\n",
       "      <td>1.5</td>\n",
       "      <td>86.975</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>73.3</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XXXX-19855</td>\n",
       "      <td>1.5</td>\n",
       "      <td>86.1</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>70.8</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XXXX-04638</td>\n",
       "      <td>1.5</td>\n",
       "      <td>85.5</td>\n",
       "      <td>50</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>80.8</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XXXX-01404</td>\n",
       "      <td>1.75</td>\n",
       "      <td>84.075</td>\n",
       "      <td>55</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>79.1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XXXX-52380</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.35</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>84.2</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XXXX-23332</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.075</td>\n",
       "      <td>55</td>\n",
       "      <td>88</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>69.1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>XXXX-60910</td>\n",
       "      <td>2.25</td>\n",
       "      <td>75.425</td>\n",
       "      <td>55</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>52.5</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XXXX-06706</td>\n",
       "      <td>2.25</td>\n",
       "      <td>74.35</td>\n",
       "      <td>55</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XXXX-21128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>72.55</td>\n",
       "      <td>55</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>XXXX-78356</td>\n",
       "      <td>2.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>50</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>65</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XXXX-10236</td>\n",
       "      <td>2.75</td>\n",
       "      <td>66.4</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>100</td>\n",
       "      <td>46.6</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XXXX-12034</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.375</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>52.5</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XXXX-21123</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>51.4</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>XXXX-90339</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58.45</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>70.8</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>XXXX-98340</td>\n",
       "      <td>5.0</td>\n",
       "      <td>53.425</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "      <td>63.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XXXX-21122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>25.8</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>XXXX-46106</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.625</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>72.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XXXX-62543</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.75</td>\n",
       "      <td>50</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>55.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XXXX-36198</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.275</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>49.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XXXX-67839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>74.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>XXXX-26390</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.3</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student Number Final Grade Wgtd Ave PA1 PA2 PA3 PA4 Reporting   Exam  \\\n",
       "19     XXXX-28026         1.0   96.325  95  95  95  93       100  104.1   \n",
       "3      XXXX-07128         1.0     92.6  93  95  95  95       100   84.2   \n",
       "27     XXXX-74467         1.0   92.225  93  93  90  90       100   87.5   \n",
       "15     XXXX-21426         1.0   92.175  88  90  85  83       100   98.3   \n",
       "9      XXXX-20754         1.0     92.1  88  88  90  85       100     92   \n",
       "16     XXXX-22085         1.0     92.1  95  88  95  93       100     85   \n",
       "11     XXXX-21036        1.25   90.675  97  85  88  88       100   87.5   \n",
       "10     XXXX-20911        1.25    90.05  95  88  95  95       100     75   \n",
       "6      XXXX-10667        1.25   88.025  60  88  88  90       100   91.7   \n",
       "4      XXXX-09114         1.5       87  95  88  95  90       100   65.8   \n",
       "20     XXXX-32614         1.5   86.975  93  93  95  93       100   73.3   \n",
       "8      XXXX-19855         1.5     86.1  93  90  88  88       100   70.8   \n",
       "1      XXXX-04638         1.5     85.5  50  90  90  88       100   80.8   \n",
       "0      XXXX-01404        1.75   84.075  55  90  88  85       100   79.1   \n",
       "23     XXXX-52380         2.0    81.35  95  88   0  90       100   84.2   \n",
       "17     XXXX-23332         2.0   80.075  55  88  75  85       100   69.1   \n",
       "24     XXXX-60910        2.25   75.425  55  75  85  83       100   52.5   \n",
       "2      XXXX-06706        2.25    74.35  55  75  88  88       100     55   \n",
       "14     XXXX-21128         2.5    72.55  55  90  50  93       100     55   \n",
       "28     XXXX-78356         2.5     70.3  50  70   0  88       100     65   \n",
       "5      XXXX-10236        2.75     66.4  55  55  55  55       100   46.6   \n",
       "7      XXXX-12034         3.0   64.375  90  85   0   0       100   52.5   \n",
       "13     XXXX-21123         3.0     64.1  60  50  40  50       100   51.4   \n",
       "29     XXXX-90339         5.0    58.45  40  20  10  10       100   70.8   \n",
       "30     XXXX-98340         5.0   53.425  95  93   0  88       100   63.3   \n",
       "12     XXXX-21122         5.0     39.2   0   0   0   0       100   25.8   \n",
       "22     XXXX-46106         5.0   38.625  60  45   0   0       100   72.5   \n",
       "25     XXXX-62543         5.0    37.75  50  88   0   0       100   55.8   \n",
       "21     XXXX-36198         5.0   32.275  50  50   0   0       100   49.1   \n",
       "26     XXXX-67839         5.0    28.55   0   0   0   0       100   74.2   \n",
       "18     XXXX-26390         5.0     27.3  75   0   0   0       100   39.2   \n",
       "\n",
       "   Mini-Project  \n",
       "19           90  \n",
       "3            95  \n",
       "27           95  \n",
       "15           92  \n",
       "9            96  \n",
       "16           95  \n",
       "11           92  \n",
       "10           96  \n",
       "6            90  \n",
       "4            95  \n",
       "20           85  \n",
       "8            90  \n",
       "1            94  \n",
       "0            90  \n",
       "23           92  \n",
       "17           90  \n",
       "24           90  \n",
       "2            80  \n",
       "14           80  \n",
       "28           93  \n",
       "5            91  \n",
       "7            95  \n",
       "13           85  \n",
       "29           91  \n",
       "30            0  \n",
       "12           91  \n",
       "22            0  \n",
       "25            0  \n",
       "21            0  \n",
       "26            0  \n",
       "18            0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['Wgtd Ave'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('grades.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "35\t202321128\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "14\t201398340\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "28\t201810667\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "30\t202320911\tProf. Eugene Rex L. Jalao, Ph.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"Student Number\t Adviser\n",
    "199506583\tAssoc. Prof. Karl Ezra S. Pilario, Ph.D.\n",
    "200036198\tAsst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
    "200232448\tAsst. Prof. Jaymar Soriano, D.Sc.\n",
    "200364070\tProf. Prospero C. Naval, Jr., Ph.D.\n",
    "200738537\tProf. Rhandley Cajote, Ph.D.\n",
    "200926390\tProf. Prospero C. Naval, Jr., Ph.D.\n",
    "201004328\tProf. Adrian Roy Valdez, Ph.D.\n",
    "201114811\tAsst. Prof. Paul Rossener Regonia, D.Sc.\n",
    "201162543\tProf. Prospero C. Naval, Jr., Ph.D.\n",
    "201178356\tAsst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
    "201252380\tAssoc. Prof. Karl Ezra S. Pilario, Ph.D.\n",
    "201306706\tAsst. Prof. Jaymar Soriano, D.Sc.\n",
    "201337829\tAssoc. Prof. Karl Ezra S. Pilario, Ph.D.\n",
    "201367839\tProf. Adrian Roy Valdez, Ph.D.\n",
    "201398340\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "201405189\tProf. Manuel Ramos Jr., Ph.D.\n",
    "201419855\tProf. Rowel O. Atienza, Ph.D.\n",
    "201428026\tProf. Rhandley Cajote, Ph.D.\n",
    "201431618\tAsst. Prof. Jaymar Soriano, D.Sc.\n",
    "201460910\tProf. Rhandley Cajote, Ph.D.\n",
    "201474467\tProf. Adrian Roy Valdez, Ph.D.\n",
    "201509114\tProf. Rowel O. Atienza, Ph.D.\n",
    "201510236\tAsst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
    "201512034\tProf. Rowel O. Atienza, Ph.D.\n",
    "201801404\tProf. Rowel O. Atienza, Ph.D.\n",
    "201802814\tAsst. Prof. Paul Rossener Regonia, D.Sc.\n",
    "201804638\tAsst. Prof. Paul Rossener Regonia, D.Sc.\n",
    "201808654\tProf. Anastacia B. Alvarez, Ph.D.\n",
    "201810667\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "202320754\tProf. Rhandley Cajote, Ph.D.\n",
    "202320911\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "202320913\tAssoc. Prof. Marc Rosales, Ph.D.\n",
    "202321036\tAsst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
    "202321122\tProf. Prospero C. Naval, Jr., Ph.D.\n",
    "202321123\tProf. Anastacia B. Alvarez, Ph.D.\n",
    "202321128\tProf. Eugene Rex L. Jalao, Ph.D.\n",
    "202321426\tAsst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student Number</th>\n",
       "      <th>Adviser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199506583</td>\n",
       "      <td>Assoc. Prof. Karl Ezra S. Pilario, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>201337829</td>\n",
       "      <td>Assoc. Prof. Karl Ezra S. Pilario, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>201252380</td>\n",
       "      <td>Assoc. Prof. Karl Ezra S. Pilario, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>202320913</td>\n",
       "      <td>Assoc. Prof. Marc Rosales, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>201306706</td>\n",
       "      <td>Asst. Prof. Jaymar Soriano, D.Sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>201431618</td>\n",
       "      <td>Asst. Prof. Jaymar Soriano, D.Sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200232448</td>\n",
       "      <td>Asst. Prof. Jaymar Soriano, D.Sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>202321426</td>\n",
       "      <td>Asst. Prof. Miguel Francisco M. Remolona, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>202321036</td>\n",
       "      <td>Asst. Prof. Miguel Francisco M. Remolona, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>201178356</td>\n",
       "      <td>Asst. Prof. Miguel Francisco M. Remolona, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200036198</td>\n",
       "      <td>Asst. Prof. Miguel Francisco M. Remolona, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>201510236</td>\n",
       "      <td>Asst. Prof. Miguel Francisco M. Remolona, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201114811</td>\n",
       "      <td>Asst. Prof. Paul Rossener Regonia, D.Sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>201804638</td>\n",
       "      <td>Asst. Prof. Paul Rossener Regonia, D.Sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>201802814</td>\n",
       "      <td>Asst. Prof. Paul Rossener Regonia, D.Sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>201474467</td>\n",
       "      <td>Prof. Adrian Roy Valdez, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>201004328</td>\n",
       "      <td>Prof. Adrian Roy Valdez, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>201367839</td>\n",
       "      <td>Prof. Adrian Roy Valdez, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>202321123</td>\n",
       "      <td>Prof. Anastacia B. Alvarez, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>201808654</td>\n",
       "      <td>Prof. Anastacia B. Alvarez, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>202321128</td>\n",
       "      <td>Prof. Eugene Rex L. Jalao, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201398340</td>\n",
       "      <td>Prof. Eugene Rex L. Jalao, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>201810667</td>\n",
       "      <td>Prof. Eugene Rex L. Jalao, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>202320911</td>\n",
       "      <td>Prof. Eugene Rex L. Jalao, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201405189</td>\n",
       "      <td>Prof. Manuel Ramos Jr., Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200364070</td>\n",
       "      <td>Prof. Prospero C. Naval, Jr., Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200926390</td>\n",
       "      <td>Prof. Prospero C. Naval, Jr., Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>202321122</td>\n",
       "      <td>Prof. Prospero C. Naval, Jr., Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201162543</td>\n",
       "      <td>Prof. Prospero C. Naval, Jr., Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200738537</td>\n",
       "      <td>Prof. Rhandley Cajote, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>201428026</td>\n",
       "      <td>Prof. Rhandley Cajote, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>201460910</td>\n",
       "      <td>Prof. Rhandley Cajote, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>202320754</td>\n",
       "      <td>Prof. Rhandley Cajote, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>201419855</td>\n",
       "      <td>Prof. Rowel O. Atienza, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>201801404</td>\n",
       "      <td>Prof. Rowel O. Atienza, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>201512034</td>\n",
       "      <td>Prof. Rowel O. Atienza, Ph.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>201509114</td>\n",
       "      <td>Prof. Rowel O. Atienza, Ph.D.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student Number                                          Adviser\n",
       "0       199506583         Assoc. Prof. Karl Ezra S. Pilario, Ph.D.\n",
       "12      201337829         Assoc. Prof. Karl Ezra S. Pilario, Ph.D.\n",
       "10      201252380         Assoc. Prof. Karl Ezra S. Pilario, Ph.D.\n",
       "31      202320913                 Assoc. Prof. Marc Rosales, Ph.D.\n",
       "11      201306706                Asst. Prof. Jaymar Soriano, D.Sc.\n",
       "18      201431618                Asst. Prof. Jaymar Soriano, D.Sc.\n",
       "2       200232448                Asst. Prof. Jaymar Soriano, D.Sc.\n",
       "36      202321426  Asst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
       "32      202321036  Asst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
       "9       201178356  Asst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
       "1       200036198  Asst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
       "22      201510236  Asst. Prof. Miguel Francisco M. Remolona, Ph.D.\n",
       "7       201114811         Asst. Prof. Paul Rossener Regonia, D.Sc.\n",
       "26      201804638         Asst. Prof. Paul Rossener Regonia, D.Sc.\n",
       "25      201802814         Asst. Prof. Paul Rossener Regonia, D.Sc.\n",
       "20      201474467                   Prof. Adrian Roy Valdez, Ph.D.\n",
       "6       201004328                   Prof. Adrian Roy Valdez, Ph.D.\n",
       "13      201367839                   Prof. Adrian Roy Valdez, Ph.D.\n",
       "34      202321123                Prof. Anastacia B. Alvarez, Ph.D.\n",
       "27      201808654                Prof. Anastacia B. Alvarez, Ph.D.\n",
       "35      202321128                 Prof. Eugene Rex L. Jalao, Ph.D.\n",
       "14      201398340                 Prof. Eugene Rex L. Jalao, Ph.D.\n",
       "28      201810667                 Prof. Eugene Rex L. Jalao, Ph.D.\n",
       "30      202320911                 Prof. Eugene Rex L. Jalao, Ph.D.\n",
       "15      201405189                    Prof. Manuel Ramos Jr., Ph.D.\n",
       "3       200364070              Prof. Prospero C. Naval, Jr., Ph.D.\n",
       "5       200926390              Prof. Prospero C. Naval, Jr., Ph.D.\n",
       "33      202321122              Prof. Prospero C. Naval, Jr., Ph.D.\n",
       "8       201162543              Prof. Prospero C. Naval, Jr., Ph.D.\n",
       "4       200738537                     Prof. Rhandley Cajote, Ph.D.\n",
       "17      201428026                     Prof. Rhandley Cajote, Ph.D.\n",
       "19      201460910                     Prof. Rhandley Cajote, Ph.D.\n",
       "29      202320754                     Prof. Rhandley Cajote, Ph.D.\n",
       "16      201419855                    Prof. Rowel O. Atienza, Ph.D.\n",
       "24      201801404                    Prof. Rowel O. Atienza, Ph.D.\n",
       "23      201512034                    Prof. Rowel O. Atienza, Ph.D.\n",
       "21      201509114                    Prof. Rowel O. Atienza, Ph.D."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row2 = text2.split('\\n')\n",
    "row2\n",
    "\n",
    "data2 = []\n",
    "\n",
    "for row in row2:\n",
    "    data2.append(row.split('\\t'))\n",
    "\n",
    "data2 = data2[1:-1]\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns=['Student Number', 'Adviser'])\n",
    "df2.sort_values(by=['Adviser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
